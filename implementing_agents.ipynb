{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhimnexus/bhimnexus/blob/main/implementing_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (quiet)\n",
        "!pip install -q termcolor langchain-openai langchain openai langchain_experimental tiktoken python-docx\n",
        "!pip install -q faiss-cpu==1.9.0.post1"
      ],
      "metadata": {
        "id": "zlld-zLayWuW"
      },
      "id": "zlld-zLayWuW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Option A - set directly (only if this notebook is private)\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "\n",
        "# Option B - interactive (safer)\n",
        "from getpass import getpass\n",
        "key = getpass(\"OpenAI API key (input will be hidden): \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = key\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgFvxJJ7yYn6",
        "outputId": "9d799d1a-9b6b-46da-ad48-8f35eefcaabc"
      },
      "id": "OgFvxJJ7yYn6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API key (input will be hidden): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "from termcolor import colored\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# -------------------------\n",
        "# FAISS (working version)\n",
        "# -------------------------\n",
        "import faiss\n",
        "print(\"faiss version:\", faiss.__version__)\n",
        "\n",
        "# -------------------------\n",
        "# LangChain v0.1.14 + OpenAI v0.0.8 imports\n",
        "# -------------------------\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "\n",
        "# -------------------------\n",
        "# Experimental generative agents\n",
        "# (these may or may not load depending on version)\n",
        "# -------------------------\n",
        "try:\n",
        "    from langchain_experimental.generative_agents import (\n",
        "        GenerativeAgent,\n",
        "        GenerativeAgentMemory,\n",
        "    )\n",
        "    print(\"GenerativeAgent import OK.\")\n",
        "except Exception as e:\n",
        "    print(\"GenerativeAgent import failed:\", e)\n",
        "\n",
        "print(\"\\nImports OK. Python version:\", sys.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJgwWiy6ycW_",
        "outputId": "2e685bf9-7e75-4d42-9643-1cd9ff7d2477"
      },
      "id": "yJgwWiy6ycW_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "faiss version: 1.9.0\n",
            "GenerativeAgent import OK.\n",
            "\n",
            "Imports OK. Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your uploaded resume\n",
        "file_path = \"/content/Bhim_Singh_Senior COBOL_Developer_Resume.docx\"\n",
        "print(\"Loading:\", file_path)\n",
        "\n",
        "from docx import Document as DocxDocument\n",
        "\n",
        "def load_docx(path):\n",
        "    doc = DocxDocument(path)\n",
        "    paragraphs = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]\n",
        "    return \"\\n\\n\".join(paragraphs)\n",
        "\n",
        "raw_text = load_docx(file_path)\n",
        "\n",
        "print(\"Loaded text length:\", len(raw_text), \"characters\")\n",
        "print(\"\\n--- Resume Preview (first 1000 chars) ---\\n\")\n",
        "print(raw_text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glG_P3Y7yrFC",
        "outputId": "abb98df3-5d3e-457b-dd22-d87ff2c90324"
      },
      "id": "glG_P3Y7yrFC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading: /content/Bhim_Singh_Senior COBOL_Developer_Resume.docx\n",
            "Loaded text length: 2808 characters\n",
            "\n",
            "--- Resume Preview (first 1000 chars) ---\n",
            "\n",
            "BHIM SINGH\n",
            "\n",
            "üìû +91 9810811949 | ‚úâÔ∏è bhim.sb@gmail.com | üåê www.linkedin.com/in/bhim-singh-mainframelead\n",
            "\n",
            "PROFILE SUMMARY\n",
            "\n",
            "Senior Mainframe Developer with 13+ years of hands-on experience in designing, coding, testing, and maintaining large-scale Mainframe applications using COBOL, JCL, VSAM, DB2, and CICS across banking, insurance, and healthcare domains. Skilled in performance tuning, production support, batch optimization, and end-to-end SDLC delivery. Strong problem-solver with proven ability to deliver stable, high-performing mainframe systems aligned with business and SLA targets.\n",
            "\n",
            "CORE TECHNICAL SKILLS\n",
            "\n",
            "Languages: COBOL, JCL, PL/I, ASSEMBLER, REXX\n",
            "Databases: DB2, VSAM\n",
            "Online Systems: CICS\n",
            "Tools: ChangeMan, Endeavor, File-AID, Abend-AID, Expeditor, Control-M, CA7, ServiceNow, Remedy\n",
            "Methodologies: Agile (Scrum, Kanban), Waterfall, DevOps\n",
            "Domains: Payments & Cards, Insurance, Healthcare, Travel\n",
            "\n",
            "PROFESSIONAL EXPERIENCE\n",
            "\n",
            "TATA CONSULTANCY SERVICES (TCS) ‚Äî Technical Lead / Senior Support\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"\\nUploaded files:\")\n",
        "for fn in uploaded.keys():\n",
        "    print(fn)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "sasAmFbo4a5w",
        "outputId": "9b376e83-a2c7-49a1-f9f3-67b645d4b364"
      },
      "id": "sasAmFbo4a5w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8ad1b1eb-8e79-434e-9161-cd237aab9b06\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8ad1b1eb-8e79-434e-9161-cd237aab9b06\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Bhim_Singh_Senior COBOL_Developer_Resume.docx to Bhim_Singh_Senior COBOL_Developer_Resume.docx\n",
            "\n",
            "Uploaded files:\n",
            "Bhim_Singh_Senior COBOL_Developer_Resume.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yss1Im4C5V6q"
      },
      "id": "Yss1Im4C5V6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunking helper\n",
        "def chunk_text(text, chunk_size=1200, overlap=200):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# Create chunks\n",
        "chunks = chunk_text(raw_text, chunk_size=1200, overlap=200)\n",
        "print(f\"Total chunks created: {len(chunks)}\")\n",
        "\n",
        "# Convert chunks to LangChain Document objects\n",
        "from langchain.schema import Document\n",
        "documents = [Document(page_content=c, metadata={\"source\": file_path, \"chunk_id\": i})\n",
        "             for i, c in enumerate(chunks)]\n",
        "\n",
        "print(\"Sample chunk preview:\\n\")\n",
        "print(documents[0].page_content[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viXATi87ytHX",
        "outputId": "c12c7b0f-1c98-4ec2-9bc7-4b6da8e80114"
      },
      "id": "viXATi87ytHX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 3\n",
            "Sample chunk preview:\n",
            "\n",
            "BHIM SINGH\n",
            "\n",
            "üìû +91 9810811949 | ‚úâÔ∏è bhim.sb@gmail.com | üåê www.linkedin.com/in/bhim-singh-mainframelead\n",
            "\n",
            "PROFILE SUMMARY\n",
            "\n",
            "Senior Mainframe Developer with 13+ years of hands-on experience in designing, coding, testing, and maintaining large-scale Mainframe applications using COBOL, JCL, VSAM, DB2, and CICS across banking, insurance, and healthcare domains. Skilled in performance tuning, production support, batch optimization, and end-to-end SDLC delivery. Strong problem-solver with proven ability to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load free embedding model\n",
        "hf_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "print(\"HuggingFace model loaded!\")\n",
        "\n",
        "\n",
        "# LangChain-compatible embedding class (FIXED)\n",
        "class HFEmbeddings:\n",
        "    def embed_documents(self, texts):\n",
        "        return hf_model.encode(texts, convert_to_numpy=True).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return hf_model.encode([text], convert_to_numpy=True)[0].tolist()\n",
        "\n",
        "    # ‚≠ê FIX: make object callable\n",
        "    def __call__(self, text):\n",
        "        return self.embed_query(text)\n",
        "\n",
        "\n",
        "embeddings = HFEmbeddings()\n",
        "\n",
        "# Prepare texts & metadata\n",
        "text_list = [d.page_content for d in documents]\n",
        "metadata_list = [d.metadata for d in documents]\n",
        "\n",
        "# Compute embeddings\n",
        "print(\"Generating embeddings...\")\n",
        "text_embeddings = embeddings.embed_documents(text_list)\n",
        "print(\"Generated\", len(text_embeddings), \"embeddings.\")\n",
        "\n",
        "# Pair text & vectors for FAISS\n",
        "text_embedding_pairs = list(zip(text_list, text_embeddings))\n",
        "\n",
        "# Build FAISS vectorstore\n",
        "vectorstore = FAISS.from_embeddings(\n",
        "    text_embeddings=text_embedding_pairs,\n",
        "    embedding=embeddings,       # FAISS will now call embeddings(text)\n",
        "    metadatas=metadata_list\n",
        ")\n",
        "\n",
        "print(\"\\nFAISS Index created successfully!\")\n",
        "print(\"Total vectors stored:\", vectorstore.index.ntotal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAejpUvayusq",
        "outputId": "08fdda20-52c8-4095-fa6b-ea95b1a8dc73"
      },
      "id": "yAejpUvayusq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HuggingFace model loaded!\n",
            "Generating embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 3 embeddings.\n",
            "\n",
            "FAISS Index created successfully!\n",
            "Total vectors stored: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n"
      ],
      "metadata": {
        "id": "sBGJO1hO7p5U"
      },
      "id": "sBGJO1hO7p5U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Experience with COBOL, JCL, production support and banking payments\"\n",
        "hits = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "for i, hit in enumerate(hits, 1):\n",
        "    header = f\"HIT {i} ‚Äî source: {hit.metadata.get('source')} chunk_id: {hit.metadata.get('chunk_id')}\"\n",
        "    print(colored(header, \"cyan\"))\n",
        "    print(hit.page_content[:800].strip() + (\"...\" if len(hit.page_content)>800 else \"\"))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHAakRMfywI5",
        "outputId": "67401eef-3199-4ee1-9cd5-29774240e0d1"
      },
      "id": "jHAakRMfywI5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HIT 1 ‚Äî source: /content/Bhim_Singh_Senior COBOL_Developer_Resume.docx chunk_id: 1\n",
            "Analyst\n",
            "\n",
            "June 2021 ‚Äì September 2025\n",
            "\n",
            "‚Ä¢ Developed and optimized COBOL and JCL modules for multiple batch and online systems, improving runtime efficiency by 20%.\n",
            "‚Ä¢ Provided end-to-end application support for critical banking and payment systems, ensuring 98% uptime.\n",
            "‚Ä¢ Debugged and resolved high-priority incidents using Abend-AID and File-AID, improving MTTR by 25%.\n",
            "‚Ä¢ Implemented preventive fixes and RCA-based improvements, reducing recurring job failures by 20%.\n",
            "‚Ä¢ Executed 15‚Äì20 year changes and deployments with 100% compliance to change management standards.\n",
            "\n",
            "HCL TECHNOLOGIES ‚Äî Senior Project Lead / Mainframe Developer\n",
            "\n",
            "December 2016 ‚Äì June 2021\n",
            "\n",
            "‚Ä¢ Designed, coded, and tested new COBOL programs and DB2 queries for batch and CICS-based systems.\n",
            "‚Ä¢ Automated repetitive JCL processes using RE...\n",
            "\n",
            "HIT 2 ‚Äî source: /content/Bhim_Singh_Senior COBOL_Developer_Resume.docx chunk_id: 2\n",
            "g batch jobs to reduce execution time.\n",
            "\n",
            "HCL TECHNOLOGIES ‚Äî Project Lead / COBOL Programmer\n",
            "\n",
            "October 2006 ‚Äì December 2016\n",
            "\n",
            "‚Ä¢ Developed and enhanced COBOL, CICS, and DB2 programs in CLOAS application.\n",
            "‚Ä¢ Resolved Control-M job failures, ensuring consistent SLA adherence and improved CSAT ratings.\n",
            "‚Ä¢ Created knowledge database and templates for code reviews and testing, boosting delivery quality.\n",
            "\n",
            "EDUCATION\n",
            "\n",
            "M.Sc. (IT) ‚Äì Kuvempu University\n",
            "GNIIT Diploma (Software Engineering) ‚Äì NIIT Ltd. (CGPA 8.0/10)\n",
            "B.Sc. (General) ‚Äì Delhi University\n",
            "\n",
            "CERTIFICATIONS\n",
            "\n",
            "‚Ä¢ Agile Master & ITSM Internal (HCL Technologies)\n",
            "‚Ä¢ Python (Basic‚ÄìAdvanced) ‚Äì Udemy\n",
            "‚Ä¢ Advanced MS Excel & Access ‚Äì New Horizons\n",
            "\n",
            "ACHIEVEMENTS\n",
            "\n",
            "‚Ä¢ Received multiple awards including Star Performer, Extraordinary AIR, Best Support Team, and WOW Reco...\n",
            "\n",
            "HIT 3 ‚Äî source: /content/Bhim_Singh_Senior COBOL_Developer_Resume.docx chunk_id: 0\n",
            "BHIM SINGH\n",
            "\n",
            "üìû +91 9810811949 | ‚úâÔ∏è bhim.sb@gmail.com | üåê www.linkedin.com/in/bhim-singh-mainframelead\n",
            "\n",
            "PROFILE SUMMARY\n",
            "\n",
            "Senior Mainframe Developer with 13+ years of hands-on experience in designing, coding, testing, and maintaining large-scale Mainframe applications using COBOL, JCL, VSAM, DB2, and CICS across banking, insurance, and healthcare domains. Skilled in performance tuning, production support, batch optimization, and end-to-end SDLC delivery. Strong problem-solver with proven ability to deliver stable, high-performing mainframe systems aligned with business and SLA targets.\n",
            "\n",
            "CORE TECHNICAL SKILLS\n",
            "\n",
            "Languages: COBOL, JCL, PL/I, ASSEMBLER, REXX\n",
            "Databases: DB2, VSAM\n",
            "Online Systems: CICS\n",
            "Tools: ChangeMan, Endeavor, File-AID, Abend-AID, Expeditor, Control-M, CA7, ServiceNow, Remedy\n",
            "Meth...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "def answer_with_retrieval_gemini(question, k=3):\n",
        "    hits = vectorstore.similarity_search(question, k=k)\n",
        "    context = \"\\n\\n---\\n\\n\".join([h.page_content for h in hits])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Use the following resume excerpts to answer the question.\n",
        "If the answer is not found, reply exactly: \"Not found in resume\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "# Test\n",
        "question = \"What mainframe technologies does Bhim Singh have experience with?\"\n",
        "print(\"Q:\", question)\n",
        "print(\"\\nA:\", answer_with_retrieval_gemini(question))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "qxHOUM6iB20y",
        "outputId": "a6ec02c7-b2aa-4930-debf-49a1e71bfa32"
      },
      "id": "qxHOUM6iB20y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What mainframe technologies does Bhim Singh have experience with?\n",
            "\n",
            "A: Bhim Singh has experience with the following mainframe technologies: COBOL, JCL, VSAM, DB2, CICS, PL/I, ASSEMBLER, REXX, ChangeMan, Endeavor, File-AID, Abend-AID, Expeditor, Control-M, CA7, ServiceNow, and Remedy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai\n"
      ],
      "metadata": {
        "id": "Z1xHLbnA__4j"
      },
      "id": "Z1xHLbnA__4j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from getpass import getpass\n",
        "\n",
        "GEMINI_KEY = getpass(\"Enter your FREE Gemini API key: \")\n",
        "genai.configure(api_key=GEMINI_KEY)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9fbVJ-XAD9K",
        "outputId": "42ef9d71-b6fb-46d8-8947-58119bd55157"
      },
      "id": "B9fbVJ-XAD9K",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your FREE Gemini API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "    print(m.name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jxNyID-UBIf2",
        "outputId": "c2987f32-6c6a-4444-c130-2bfc56e2dfac"
      },
      "id": "jxNyID-UBIf2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-3-pro-preview\n",
            "models/gemini-3-pro-image-preview\n",
            "models/nano-banana-pro-preview\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.0-flash-live-001\n",
            "models/gemini-live-2.5-flash-preview\n",
            "models/gemini-2.5-flash-live-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want TimeWeightedVectorStoreRetriever or GenerativeAgent behavior:\n",
        "print(\"Optional notes:\")\n",
        "print(\"- TimeWeightedVectorStoreRetriever API may have moved; if needed I can add a compatible example.\")\n",
        "print(\"- GenerativeAgent (experimental) imports were attempted earlier. If you want a working multi-agent example I can provide one, but it may require pinning specific langchain versions.\")\n"
      ],
      "metadata": {
        "id": "SUXjxSriyyuz"
      },
      "id": "SUXjxSriyyuz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}